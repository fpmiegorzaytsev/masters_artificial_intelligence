# Решение

## Level 1

1. Обучение Reward модели

Для обучения использовался стандартный лосс 

$$
\mathcal{L}_{{RM}} = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}} \left[ \log \sigma\left( r_\theta(y_w \mid x) - r_\theta(y_l \mid x) \right) \right] \\
\\
\mathcal{D} = \{(x, y_w, y_l)\}_{i=1}^{N}
$$

Также эксперементировал с центрированием ревардов модели, добавляя к лоссу
 
$$
\gamma \cdot [r_\theta(y_w \mid x) + r_\theta(y_l \mid x)]^2
$$

Однако это не дало значимого улучшения accuracy, поэтому в итоге от данной идеи я отказался

Accuracy на предложенных данных составило приблизительно $0.62$

Для менее затратных вычислений я ограничивал длину промпта + ответа до 512 токенов.

2. Alignment основной модели

В референсной статье авторы предлагают брать за действие генерацию не одного токена ответа модели, а генерацию всей выходной последовательности. Для уменьшения дисперсии (в данном решении) используется moving average baseline - среднее по ревардам за все тренировочные шаги.

Градиент обновления:

$$
\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot \mid x)} \left[ \left( R(y, x) - b \right) \nabla_\theta \log \pi_\theta(y \mid x) \right] = 
\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot \mid x)} \left[ \left( R(y, x) - b \right) \sum_{t=1}^T \nabla_\theta \log \pi_\theta(y_t \mid x, y_{<t}) \right]
$$

Вычисление градиентов по такому лоссу связано с большими вычислительными затратами. В torch придется складывать графы вычислений для всех генерируемых токенов, что тратит огромное количество памяти.
Я ограничивал длину генерируемой последовательности до 32 токенов, а также размер батча до 3 единиц. Так как среднее значение длин на трейн выборке равнялось примерно 200 токенам, длина промпта обрезалась по 256 токенам, чтобы чуть лучше соответствовать данным, на которых училась ревард модель.

Я пытался использовать и другие настройки (например, увеличивал длину, уменьшая размер батча) чтобы дотянуть до хотя бы средней длины генерируемой последовательности в 47 токенов(таблица 2 в препринте), однако данные попытки не увенчались успехом (все еще было OOM) и я остановился на 32 токенах.

В силу затратных вычислений я успел прогнать лишь 2 цикла полного обучения, соответственно не подбирая параметры.

Сравнение полученной модели и SFT приведено в ноутбуке `comparison_aligned vs sft.ipynb` 

## Level 2

1. Обучение Reward Модели с распределением.

Для обучения ревард модели, выдающей распределение я опирался на логику обучения скалярной ревард модели.

Коль скоро цель все также максимизировать $P(y_w \succ y_l \mid x)$, задача представляется следующим образом:

пусть $r_w \sim R(y_w \mid x), r_l \sim R(y_l \mid x)$, будем максимизировать $P(r_w > r_l \mid x)$.

Да деле это выглядит как максимизация 

$$
P(r_w > r_l \mid x) = \sum_{i=1}^{n} \sum_{j=1}^{i-1} R_i(y_w \mid x) \cdot R_j(y_l \mid x)
$$

Где взятие индекса у $R$ обозначает вероятность присвоенную реварду, равному $i$,
Распределение $R$ можно моделировать как $$R_i(y \mid x) = \frac{e^{z_i}}{\sum_{j=1}^{n}e^{z_j}}$$

где $z_i$ - логит $i$. \
Таким образом лосс будет выглядеть

$$
\mathcal{L} = - \log \sum_{i > j} R_i(y_w \mid x) R_j(y_l \mid x)
$$

На практике хотелось бы, чтобы массы распределений были размазаны по разным значениям возможных ревардом, так как это более менее соответсвует человеческой оценке. Действительно, в случае сомнения человек распределит свой голос между несколькими близко находящимися оценками (например, 7-8-9), поставив остальным очень малую вероятность (около нуля).

Хотелось бы требовать то же самое и от модели, так что к лоссу я добавляю сумму энтропий распределений, умноженную на некоторую константу.

$$
\mathcal{L_{final}} = \mathcal{L} - \lambda \cdot \sum_{i=1}^{n} \left( R_i(y_w \mid x) \log [R_i(y_w \mid x)] + R_i(y_l \mid x) \log [R_i(y_l \mid x)]\right)
$$

На предложенных данных, введение такого регуляризационного члена показало себя не очень хорошо. В конце ноутбука `reward_model_level_2.ipynb` изображены графики полученных распределений для положительных и отрицательных примеров из тренировочных данных. Некоторые распределения, действительно соответствуют упомянутым требованиям, когда другие могут иметь совершенно непохожую форму (например, sample 3 для chosen данных).

Таким образом, необходимо вводить некоторую другую резуляризацию

Подумать над тем, как интегрировать такую ревард модель в REINFORCE я толком не успел, разве что использовать математическое ожидание. Но даже для последнего случая я не успел провести эксперименты.